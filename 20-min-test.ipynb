{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74bd2b8a-5cd4-49b7-a181-7cd300792f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.utils import register_all_modules\n",
    "\n",
    "register_all_modules(init_default_scope=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a0324b7-af2f-4110-a918-6ca1539fa428",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'VideoInit is already registered in transform at __main__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmcv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TRANSFORMS, BaseTransform, to_tensor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmaction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActionDataSample\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129;43m@TRANSFORMS\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mVideoInit\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseTransform\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mtransform\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/registry/registry.py:666\u001b[0m, in \u001b[0;36mRegistry.register_module.<locals>._register\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_register\u001b[39m(module):\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/registry/registry.py:611\u001b[0m, in \u001b[0;36mRegistry._register_module\u001b[0;34m(self, module, module_name, force)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_dict:\n\u001b[1;32m    610\u001b[0m     existed_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_dict[name]\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already registered in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    612\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisted_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_dict[name] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[0;31mKeyError\u001b[0m: 'VideoInit is already registered in transform at __main__'"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "import decord\n",
    "import numpy as np\n",
    "from mmcv.transforms import TRANSFORMS, BaseTransform, to_tensor\n",
    "from mmaction.structures import ActionDataSample\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoInit(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        container = decord.VideoReader(results['filename'])\n",
    "        results['total_frames'] = len(container)\n",
    "        results['video_reader'] = container\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoSample(BaseTransform):\n",
    "    def __init__(self, clip_len, num_clips, test_mode=False):\n",
    "        self.clip_len = clip_len\n",
    "        self.num_clips = num_clips\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def transform(self, results):\n",
    "        total_frames = results['total_frames']\n",
    "        interval = total_frames // self.clip_len\n",
    "\n",
    "        if self.test_mode:\n",
    "            # Make the sampling during testing deterministic\n",
    "            np.random.seed(42)\n",
    "\n",
    "        inds_of_all_clips = []\n",
    "        for i in range(self.num_clips):\n",
    "            bids = np.arange(self.clip_len) * interval\n",
    "            offset = np.random.randint(interval, size=bids.shape)\n",
    "            inds = bids + offset\n",
    "            inds_of_all_clips.append(inds)\n",
    "\n",
    "        results['frame_inds'] = np.concatenate(inds_of_all_clips)\n",
    "        results['clip_len'] = self.clip_len\n",
    "        results['num_clips'] = self.num_clips\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoDecode(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        frame_inds = results['frame_inds']\n",
    "        container = results['video_reader']\n",
    "\n",
    "        imgs = container.get_batch(frame_inds).asnumpy()\n",
    "        imgs = list(imgs)\n",
    "\n",
    "        results['video_reader'] = None\n",
    "        del container\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoResize(BaseTransform):\n",
    "    def __init__(self, r_size):\n",
    "        self.r_size = (np.inf, r_size)\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        new_w, new_h = mmcv.rescale_size((img_w, img_h), self.r_size)\n",
    "\n",
    "        imgs = [mmcv.imresize(img, (new_w, new_h))\n",
    "                for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoCrop(BaseTransform):\n",
    "    def __init__(self, c_size):\n",
    "        self.c_size = c_size\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        center_x, center_y = img_w // 2, img_h // 2\n",
    "        x1, x2 = center_x - self.c_size // 2, center_x + self.c_size // 2\n",
    "        y1, y2 = center_y - self.c_size // 2, center_y + self.c_size // 2\n",
    "        imgs = [img[y1:y2, x1:x2] for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoFormat(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        num_clips = results['num_clips']\n",
    "        clip_len = results['clip_len']\n",
    "        imgs = results['imgs']\n",
    "\n",
    "        # [num_clips*clip_len, H, W, C]\n",
    "        imgs = np.array(imgs)\n",
    "        # [num_clips, clip_len, H, W, C]\n",
    "        imgs = imgs.reshape((num_clips, clip_len) + imgs.shape[1:])\n",
    "        # [num_clips, C, clip_len, H, W]\n",
    "        imgs = imgs.transpose(0, 4, 1, 2, 3)\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        return results\n",
    "\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoPack(BaseTransform):\n",
    "    def __init__(self, meta_keys=('img_shape', 'num_clips', 'clip_len')):\n",
    "        self.meta_keys = meta_keys\n",
    "\n",
    "    def transform(self, results):\n",
    "        packed_results = dict()\n",
    "        inputs = to_tensor(results['imgs'])\n",
    "        data_sample = ActionDataSample()\n",
    "        data_sample.set_gt_label(results['label'])\n",
    "        metainfo = {k: results[k] for k in self.meta_keys if k in results}\n",
    "        data_sample.set_metainfo(metainfo)\n",
    "        packed_results['inputs'] = inputs\n",
    "        packed_results['data_samples'] = data_sample\n",
    "        return packed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f49acf44-1975-4901-b8a0-eb69cbefedda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the inputs:  torch.Size([1, 3, 16, 224, 224])\n",
      "image_shape:  (224, 224)\n",
      "num_clips:  1\n",
      "clip_len:  16\n",
      "label:  tensor([0])\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from mmengine.dataset import Compose\n",
    "\n",
    "pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "pipeline = Compose(pipeline_cfg)\n",
    "data_prefix = 'data/kinetics400_tiny/train'\n",
    "results = dict(filename=osp.join(data_prefix, 'D32_1gwq35E.mp4'), label=0)\n",
    "packed_results = pipeline(results)\n",
    "\n",
    "inputs = packed_results['inputs']\n",
    "data_sample = packed_results['data_samples']\n",
    "\n",
    "print('shape of the inputs: ', inputs.shape)\n",
    "\n",
    "# Get metainfo of the inputs\n",
    "print('image_shape: ', data_sample.img_shape)\n",
    "print('num_clips: ', data_sample.num_clips)\n",
    "print('clip_len: ', data_sample.clip_len)\n",
    "\n",
    "# Get label of the inputs\n",
    "print('label: ', data_sample.gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdde34a4-c5b2-4230-8eb0-e93a49700dad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DatasetZelda is already registered in dataset at __main__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDataset\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmaction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DATASETS\n\u001b[1;32m      7\u001b[0m \u001b[38;5;129;43m@DATASETS\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mDatasetZelda\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseDataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mann_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodality\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodality\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/registry/registry.py:666\u001b[0m, in \u001b[0;36mRegistry.register_module.<locals>._register\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_register\u001b[39m(module):\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/registry/registry.py:611\u001b[0m, in \u001b[0;36mRegistry._register_module\u001b[0;34m(self, module, module_name, force)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_dict:\n\u001b[1;32m    610\u001b[0m     existed_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_dict[name]\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already registered in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    612\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisted_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_dict[name] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DatasetZelda is already registered in dataset at __main__'"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from mmengine.fileio import list_from_file\n",
    "from mmengine.dataset import BaseDataset\n",
    "from mmaction.registry import DATASETS\n",
    "\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class DatasetZelda(BaseDataset):\n",
    "    def __init__(self, ann_file, pipeline, data_root, data_prefix=dict(video=''),\n",
    "                 test_mode=False, modality='RGB', **kwargs):\n",
    "        self.modality = modality\n",
    "        super(DatasetZelda, self).__init__(ann_file=ann_file, pipeline=pipeline, data_root=data_root,\n",
    "                                           data_prefix=data_prefix, test_mode=test_mode,\n",
    "                                           **kwargs)\n",
    "\n",
    "    def load_data_list(self):\n",
    "        data_list = []\n",
    "        fin = list_from_file(self.ann_file)\n",
    "        for line in fin:\n",
    "            line_split = line.strip().split()\n",
    "            filename, label = line_split\n",
    "            label = int(label)\n",
    "            filename = osp.join(self.data_prefix['video'], filename)\n",
    "            data_list.append(dict(filename=filename, label=label))\n",
    "        return data_list\n",
    "\n",
    "    def get_data_info(self, idx: int) -> dict:\n",
    "        data_info = super().get_data_info(idx)\n",
    "        data_info['modality'] = self.modality\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aff558c-fe74-4088-bd96-20f704d07990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the inputs:  torch.Size([1, 3, 16, 224, 224])\n",
      "image_shape:  (224, 224)\n",
      "num_clips:  1\n",
      "clip_len:  16\n",
      "label:  tensor([0])\n"
     ]
    }
   ],
   "source": [
    "from mmaction.registry import DATASETS\n",
    "\n",
    "train_pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "val_pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=5, test_mode=True),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "train_dataset_cfg = dict(\n",
    "    type='DatasetZelda',\n",
    "    ann_file='kinetics_tiny_train_video.txt',\n",
    "    pipeline=train_pipeline_cfg,\n",
    "    data_root='data/kinetics400_tiny/',\n",
    "    data_prefix=dict(video='train'))\n",
    "\n",
    "val_dataset_cfg = dict(\n",
    "    type='DatasetZelda',\n",
    "    ann_file='kinetics_tiny_val_video.txt',\n",
    "    pipeline=val_pipeline_cfg,\n",
    "    data_root='data/kinetics400_tiny/',\n",
    "    data_prefix=dict(video='val'))\n",
    "\n",
    "train_dataset = DATASETS.build(train_dataset_cfg)\n",
    "\n",
    "packed_results = train_dataset[0]\n",
    "\n",
    "inputs = packed_results['inputs']\n",
    "data_sample = packed_results['data_samples']\n",
    "\n",
    "print('shape of the inputs: ', inputs.shape)\n",
    "\n",
    "# Get metainfo of the inputs\n",
    "print('image_shape: ', data_sample.img_shape)\n",
    "print('num_clips: ', data_sample.num_clips)\n",
    "print('clip_len: ', data_sample.clip_len)\n",
    "\n",
    "# Get label of the inputs\n",
    "print('label: ', data_sample.gt_label)\n",
    "\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataloader_cfg = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    dataset=train_dataset_cfg)\n",
    "\n",
    "val_dataloader_cfg = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    persistent_workers=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=val_dataset_cfg)\n",
    "\n",
    "train_data_loader = Runner.build_dataloader(dataloader=train_dataloader_cfg)\n",
    "val_data_loader = Runner.build_dataloader(dataloader=val_dataloader_cfg)\n",
    "\n",
    "batched_packed_results = next(iter(train_data_loader))\n",
    "\n",
    "batched_inputs = batched_packed_results['inputs']\n",
    "batched_data_sample = batched_packed_results['data_samples']\n",
    "\n",
    "assert len(batched_inputs) == BATCH_SIZE\n",
    "assert len(batched_data_sample) == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f575ae-b72b-41a0-a8b6-773aeea334f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'DataPreprocessorZelda is already registered in model at __main__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDataPreprocessor, stack_batch\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmaction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODELS\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129;43m@MODELS\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mDataPreprocessorZelda\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseDataPreprocessor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/registry/registry.py:666\u001b[0m, in \u001b[0;36mRegistry.register_module.<locals>._register\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_register\u001b[39m(module):\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/registry/registry.py:611\u001b[0m, in \u001b[0;36mRegistry._register_module\u001b[0;34m(self, module, module_name, force)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_dict:\n\u001b[1;32m    610\u001b[0m     existed_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_dict[name]\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already registered in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    612\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisted_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_dict[name] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DataPreprocessorZelda is already registered in model at __main__'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mmengine.model import BaseDataPreprocessor, stack_batch\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class DataPreprocessorZelda(BaseDataPreprocessor):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\n",
    "            'mean',\n",
    "            torch.tensor(mean, dtype=torch.float32).view(-1, 1, 1, 1),\n",
    "            False)\n",
    "        self.register_buffer(\n",
    "            'std',\n",
    "            torch.tensor(std, dtype=torch.float32).view(-1, 1, 1, 1),\n",
    "            False)\n",
    "\n",
    "    def forward(self, data, training=False):\n",
    "        data = self.cast_data(data)\n",
    "        inputs = data['inputs']\n",
    "        batch_inputs = stack_batch(inputs)  # Batching\n",
    "        batch_inputs = (batch_inputs - self.mean) / self.std  # Normalization\n",
    "        data['inputs'] = batch_inputs\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b25f7074-3ec5-4483-8aa3-07038925529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 16, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from mmaction.registry import MODELS\n",
    "\n",
    "data_preprocessor_cfg = dict(\n",
    "    type='DataPreprocessorZelda',\n",
    "    mean=[123.675, 116.28, 103.53],\n",
    "    std=[58.395, 57.12, 57.375])\n",
    "\n",
    "data_preprocessor = MODELS.build(data_preprocessor_cfg)\n",
    "\n",
    "preprocessed_inputs = data_preprocessor(batched_packed_results)\n",
    "print(preprocessed_inputs['inputs'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c303d8-596f-4798-a13a-cc7fe7fc8fc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'BackBoneZelda is already registered in model at __main__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelData\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmaction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODELS\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129;43m@MODELS\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mBackBoneZelda\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseModule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_cfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/registry/registry.py:666\u001b[0m, in \u001b[0;36mRegistry.register_module.<locals>._register\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_register\u001b[39m(module):\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmengine/registry/registry.py:611\u001b[0m, in \u001b[0;36mRegistry._register_module\u001b[0;34m(self, module, module_name, force)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_dict:\n\u001b[1;32m    610\u001b[0m     existed_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_dict[name]\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already registered in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    612\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisted_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_dict[name] \u001b[38;5;241m=\u001b[39m module\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BackBoneZelda is already registered in model at __main__'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mmengine.model import BaseModel, BaseModule, Sequential\n",
    "from mmengine.structures import LabelData\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class BackBoneZelda(BaseModule):\n",
    "    def __init__(self, init_cfg=None):\n",
    "        if init_cfg is None:\n",
    "            init_cfg = [dict(type='Kaiming', layer='Conv3d', mode='fan_out', nonlinearity=\"relu\"),\n",
    "                        dict(type='Constant', layer='BatchNorm3d', val=1, bias=0)]\n",
    "\n",
    "        super(BackBoneZelda, self).__init__(init_cfg=init_cfg)\n",
    "\n",
    "        self.conv1 = Sequential(nn.Conv3d(3, 64, kernel_size=(3, 7, 7),\n",
    "                                          stride=(1, 2, 2), padding=(1, 3, 3)),\n",
    "                                nn.BatchNorm3d(64), nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2),\n",
    "                                    padding=(0, 1, 1))\n",
    "\n",
    "        self.conv = Sequential(nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                               nn.BatchNorm3d(128), nn.ReLU())\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        # imgs: [batch_size*num_views, 3, T, H, W]\n",
    "        # features: [batch_size*num_views, 128, T/2, H//8, W//8]\n",
    "        features = self.conv(self.maxpool(self.conv1(imgs)))\n",
    "        return features\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class ClsHeadZelda(BaseModule):\n",
    "    def __init__(self, num_classes, in_channels, dropout=0.5, average_clips='prob', init_cfg=None):\n",
    "        if init_cfg is None:\n",
    "            init_cfg = dict(type='Normal', layer='Linear', std=0.01)\n",
    "\n",
    "        super(ClsHeadZelda, self).__init__(init_cfg=init_cfg)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.average_clips = average_clips\n",
    "\n",
    "        if dropout != 0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "        self.fc = nn.Linear(self.in_channels, self.num_classes)\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, T, H, W = x.shape\n",
    "        x = self.pool(x)\n",
    "        x = x.view(N, C)\n",
    "        assert x.shape[1] == self.in_channels\n",
    "\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        cls_scores = self.fc(x)\n",
    "        return cls_scores\n",
    "\n",
    "    def loss(self, feats, data_samples):\n",
    "        cls_scores = self(feats)\n",
    "        labels = torch.stack([x.gt_label for x in data_samples])\n",
    "        labels = labels.squeeze()\n",
    "\n",
    "        if labels.shape == torch.Size([]):\n",
    "            labels = labels.unsqueeze(0)\n",
    "\n",
    "        loss_cls = self.loss_fn(cls_scores, labels)\n",
    "        return dict(loss_cls=loss_cls)\n",
    "\n",
    "    def predict(self, feats, data_samples):\n",
    "        cls_scores = self(feats)\n",
    "        num_views = cls_scores.shape[0] // len(data_samples)\n",
    "        # assert num_views == data_samples[0].num_clips\n",
    "        cls_scores = self.average_clip(cls_scores, num_views)\n",
    "\n",
    "        for ds, sc in zip(data_samples, cls_scores):\n",
    "            pred = LabelData(item=sc)\n",
    "            ds.pred_scores = pred\n",
    "        return data_samples\n",
    "\n",
    "    def average_clip(self, cls_scores, num_views):\n",
    "          if self.average_clips not in ['score', 'prob', None]:\n",
    "            raise ValueError(f'{self.average_clips} is not supported. '\n",
    "                             f'Currently supported ones are '\n",
    "                             f'[\"score\", \"prob\", None]')\n",
    "\n",
    "          total_views = cls_scores.shape[0]\n",
    "          cls_scores = cls_scores.view(total_views // num_views, num_views, -1)\n",
    "\n",
    "          if self.average_clips is None:\n",
    "              return cls_scores\n",
    "          elif self.average_clips == 'prob':\n",
    "              cls_scores = F.softmax(cls_scores, dim=2).mean(dim=1)\n",
    "          elif self.average_clips == 'score':\n",
    "              cls_scores = cls_scores.mean(dim=1)\n",
    "\n",
    "          return cls_scores\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class RecognizerZelda(BaseModel):\n",
    "    def __init__(self, backbone, cls_head, data_preprocessor):\n",
    "        super().__init__(data_preprocessor=data_preprocessor)\n",
    "\n",
    "        self.backbone = MODELS.build(backbone)\n",
    "        self.cls_head = MODELS.build(cls_head)\n",
    "\n",
    "    def extract_feat(self, inputs):\n",
    "        inputs = inputs.view((-1, ) + inputs.shape[2:])\n",
    "        return self.backbone(inputs)\n",
    "\n",
    "    def loss(self, inputs, data_samples):\n",
    "        feats = self.extract_feat(inputs)\n",
    "        loss = self.cls_head.loss(feats, data_samples)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, inputs, data_samples):\n",
    "        feats = self.extract_feat(inputs)\n",
    "        predictions = self.cls_head.predict(feats, data_samples)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, inputs, data_samples=None, mode='tensor'):\n",
    "        if mode == 'tensor':\n",
    "            return self.extract_feat(inputs)\n",
    "        elif mode == 'loss':\n",
    "            return self.loss(inputs, data_samples)\n",
    "        elif mode == 'predict':\n",
    "            return self.predict(inputs, data_samples)\n",
    "        else:\n",
    "            raise RuntimeError(f'Invalid mode: {mode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ec5d8ed-2079-4fd9-8fca-262c1c537d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "backbone.conv1.0.weight - torch.Size([64, 3, 3, 7, 7]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "backbone.conv1.0.bias - torch.Size([64]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "backbone.conv1.1.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "backbone.conv1.1.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "backbone.conv.0.weight - torch.Size([128, 64, 3, 3, 3]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "backbone.conv.0.bias - torch.Size([128]): \n",
      "KaimingInit: a=0, mode=fan_out, nonlinearity=relu, distribution =normal, bias=0 \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "backbone.conv.1.weight - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "backbone.conv.1.bias - torch.Size([128]): \n",
      "The value is the same before and after calling `init_weights` of RecognizerZelda  \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "cls_head.fc.weight - torch.Size([2, 128]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "05/30 21:58:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "cls_head.fc.bias - torch.Size([2]): \n",
      "NormalInit: mean=0, std=0.01, bias=0 \n",
      " \n",
      "loss dict:  {'loss_cls': tensor(0.8010, grad_fn=<NllLossBackward0>)}\n",
      "Label of Sample[0] tensor([1])\n",
      "Scores of Sample[0] <LabelData(\n",
      "\n",
      "    META INFORMATION\n",
      "\n",
      "    DATA FIELDS\n",
      "    item: tensor([0.5158, 0.4842])\n",
      ") at 0x7faee3c4a2e0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from mmaction.registry import MODELS\n",
    "\n",
    "model_cfg = dict(\n",
    "    type='RecognizerZelda',\n",
    "    backbone=dict(type='BackBoneZelda'),\n",
    "    cls_head=dict(\n",
    "        type='ClsHeadZelda',\n",
    "        num_classes=2,\n",
    "        in_channels=128,\n",
    "        average_clips='prob'),\n",
    "    data_preprocessor = dict(\n",
    "        type='DataPreprocessorZelda',\n",
    "        mean=[123.675, 116.28, 103.53],\n",
    "        std=[58.395, 57.12, 57.375]))\n",
    "\n",
    "model = MODELS.build(model_cfg)\n",
    "\n",
    "# Train\n",
    "model.train()\n",
    "model.init_weights()\n",
    "data_batch_train = copy.deepcopy(batched_packed_results)\n",
    "data = model.data_preprocessor(data_batch_train, training=True)\n",
    "loss = model(**data, mode='loss')\n",
    "print('loss dict: ', loss)\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    data_batch_test = copy.deepcopy(batched_packed_results)\n",
    "    data = model.data_preprocessor(data_batch_test, training=False)\n",
    "    predictions = model(**data, mode='predict')\n",
    "print('Label of Sample[0]', predictions[0].gt_label)\n",
    "print('Scores of Sample[0]', predictions[0].pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "477d2b1e-0a34-43d8-a004-41103d7c9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import OrderedDict\n",
    "from mmengine.evaluator import BaseMetric\n",
    "from mmaction.evaluation import top_k_accuracy\n",
    "from mmaction.registry import METRICS\n",
    "\n",
    "\n",
    "@METRICS.register_module()\n",
    "class AccuracyMetric(BaseMetric):\n",
    "    def __init__(self, topk=(1, 5), collect_device='cpu', prefix='acc'):\n",
    "        super().__init__(collect_device=collect_device, prefix=prefix)\n",
    "        self.topk = topk\n",
    "\n",
    "    def process(self, data_batch, data_samples):\n",
    "        data_samples = copy.deepcopy(data_samples)\n",
    "        for data_sample in data_samples:\n",
    "            result = dict()\n",
    "            #scores = data_sample['pred_score'].cpu().numpy()\n",
    "            #label = data_sample['gt_label'].item()\n",
    "            scores = data_sample['pred_scores']['item'].cpu().numpy()\n",
    "            label = data_sample['gt_label'].item()\n",
    "            result['scores'] = scores\n",
    "            result['label'] = label\n",
    "            self.results.append(result)\n",
    "\n",
    "    def compute_metrics(self, results: list) -> dict:\n",
    "        eval_results = OrderedDict()\n",
    "        labels = [res['label'] for res in results]\n",
    "        scores = [res['scores'] for res in results]\n",
    "        topk_acc = top_k_accuracy(scores, labels, self.topk)\n",
    "        for k, acc in zip(self.topk, topk_acc):\n",
    "            eval_results[f'topk{k}'] = acc\n",
    "        return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5186173-53a2-42fc-9eb9-399ecffa3929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('topk1', 0.0), ('topk5', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "from mmaction.registry import METRICS\n",
    "\n",
    "metric_cfg = dict(type='AccuracyMetric', topk=(1, 5))\n",
    "\n",
    "metric = METRICS.build(metric_cfg)\n",
    "\n",
    "data_samples = [d.to_dict() for d in predictions]\n",
    "\n",
    "metric.process(batched_packed_results, data_samples)\n",
    "acc = metric.compute_metrics(metric.results)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8806d525-dad5-48d0-af9c-c8f6c85c265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda'  # or 'cpu'\n",
    "max_epochs = 10\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for data_batch in tqdm(train_data_loader):\n",
    "        data = model.data_preprocessor(data_batch, training=True)\n",
    "        loss_dict = model(**data, mode='loss')\n",
    "        loss = loss_dict['loss_cls']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print(f'Epoch[{epoch}]: loss ', sum(losses) / len(train_data_loader))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_batch in tqdm(val_data_loader):\n",
    "            data = model.data_preprocessor(data_batch, training=False)\n",
    "            predictions = model(**data, mode='predict')\n",
    "            data_samples = [d.to_dict() for d in predictions]\n",
    "            metric.process(data_batch, data_samples)\n",
    "\n",
    "        acc = metric.compute_metrics(metric.results)\n",
    "        for name, topk in acc.items():\n",
    "            print(f'{name}: ', topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb299ddb-affb-4a41-95bc-27c7b47c584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea201a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openmmlab)",
   "language": "python",
   "name": "openmmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
