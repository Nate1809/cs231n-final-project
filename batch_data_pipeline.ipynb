{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418af5e2-edb8-4659-bb6a-9891f9413083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiewu/miniconda3/envs/openmmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/transformer.py:33: UserWarning: Fail to import ``MultiScaleDeformableAttention`` from ``mmcv.ops.multi_scale_deform_attn``, You should install ``mmcv`` rather than ``mmcv-lite`` if you need this module. \n",
      "  warnings.warn('Fail to import ``MultiScaleDeformableAttention`` from '\n"
     ]
    }
   ],
   "source": [
    "from mmaction.utils import register_all_modules\n",
    "\n",
    "register_all_modules(init_default_scope=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f49ed34-fed5-439d-b213-02b0ef285be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "import decord\n",
    "import numpy as np\n",
    "from mmcv.transforms import TRANSFORMS, BaseTransform, to_tensor\n",
    "from mmaction.structures import ActionDataSample\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoInit(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        container = decord.VideoReader(results['filename'])\n",
    "        results['total_frames'] = len(container)\n",
    "        results['video_reader'] = container\n",
    "        return results\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoSample(BaseTransform):\n",
    "    def __init__(self, clip_len, num_clips, test_mode=False):\n",
    "        self.clip_len = clip_len\n",
    "        self.num_clips = num_clips\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "    def transform(self, results):\n",
    "        total_frames = results['total_frames']\n",
    "        interval = total_frames // self.clip_len\n",
    "\n",
    "        if self.test_mode:\n",
    "            # Make the sampling during testing deterministic\n",
    "            np.random.seed(42)\n",
    "\n",
    "        inds_of_all_clips = []\n",
    "        for i in range(self.num_clips):\n",
    "            bids = np.arange(self.clip_len) * interval\n",
    "            offset = np.random.randint(interval, size=bids.shape)\n",
    "            inds = bids + offset\n",
    "            inds_of_all_clips.append(inds)\n",
    "\n",
    "        results['frame_inds'] = np.concatenate(inds_of_all_clips)\n",
    "        results['clip_len'] = self.clip_len\n",
    "        results['num_clips'] = self.num_clips\n",
    "        return results\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoDecode(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        frame_inds = results['frame_inds']\n",
    "        container = results['video_reader']\n",
    "\n",
    "        imgs = container.get_batch(frame_inds).asnumpy()\n",
    "        imgs = list(imgs)\n",
    "\n",
    "        results['video_reader'] = None\n",
    "        del container\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoResize(BaseTransform):\n",
    "    def __init__(self, r_size):\n",
    "        self.r_size = (np.inf, r_size)\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        new_w, new_h = mmcv.rescale_size((img_w, img_h), self.r_size)\n",
    "\n",
    "        imgs = [mmcv.imresize(img, (new_w, new_h))\n",
    "                for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoCrop(BaseTransform):\n",
    "    def __init__(self, c_size):\n",
    "        self.c_size = c_size\n",
    "\n",
    "    def transform(self, results):\n",
    "        img_h, img_w = results['img_shape']\n",
    "        center_x, center_y = img_w // 2, img_h // 2\n",
    "        x1, x2 = center_x - self.c_size // 2, center_x + self.c_size // 2\n",
    "        y1, y2 = center_y - self.c_size // 2, center_y + self.c_size // 2\n",
    "        imgs = [img[y1:y2, x1:x2] for img in results['imgs']]\n",
    "        results['imgs'] = imgs\n",
    "        results['img_shape'] = imgs[0].shape[:2]\n",
    "        return results\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoFormat(BaseTransform):\n",
    "    def transform(self, results):\n",
    "        num_clips = results['num_clips']\n",
    "        clip_len = results['clip_len']\n",
    "        imgs = results['imgs']\n",
    "\n",
    "        # [num_clips*clip_len, H, W, C]\n",
    "        imgs = np.array(imgs)\n",
    "        # [num_clips, clip_len, H, W, C]\n",
    "        imgs = imgs.reshape((num_clips, clip_len) + imgs.shape[1:])\n",
    "        # [num_clips, C, clip_len, H, W]\n",
    "        imgs = imgs.transpose(0, 4, 1, 2, 3)\n",
    "\n",
    "        results['imgs'] = imgs\n",
    "        return results\n",
    "\n",
    "@TRANSFORMS.register_module()\n",
    "class VideoPack(BaseTransform):\n",
    "    def __init__(self, meta_keys=('img_shape', 'num_clips', 'clip_len')):\n",
    "        self.meta_keys = meta_keys\n",
    "\n",
    "    def transform(self, results):\n",
    "        packed_results = dict()\n",
    "        inputs = to_tensor(results['imgs'])\n",
    "        data_sample = ActionDataSample()\n",
    "        data_sample.set_gt_label(results['label'])\n",
    "        metainfo = {k: results[k] for k in self.meta_keys if k in results}\n",
    "        data_sample.set_metainfo(metainfo)\n",
    "        packed_results['inputs'] = inputs\n",
    "        packed_results['data_samples'] = data_sample\n",
    "        return packed_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c3732b-9009-48ab-81bd-885169b0df0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the inputs:  torch.Size([1, 3, 16, 224, 224])\n",
      "image_shape:  (224, 224)\n",
      "num_clips:  1\n",
      "clip_len:  16\n",
      "label:  tensor([0])\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from mmengine.dataset import Compose\n",
    "\n",
    "pipeline_cfg = [\n",
    "    dict(type='VideoInit'),\n",
    "    dict(type='VideoSample', clip_len=16, num_clips=1, test_mode=False),\n",
    "    dict(type='VideoDecode'),\n",
    "    dict(type='VideoResize', r_size=256),\n",
    "    dict(type='VideoCrop', c_size=224),\n",
    "    dict(type='VideoFormat'),\n",
    "    dict(type='VideoPack')\n",
    "]\n",
    "\n",
    "pipeline = Compose(pipeline_cfg)\n",
    "data_prefix = 'data/kinetics400_tiny/train'\n",
    "results = dict(filename=osp.join(data_prefix, 'D32_1gwq35E.mp4'), label=0)\n",
    "packed_results = pipeline(results)\n",
    "\n",
    "inputs = packed_results['inputs']\n",
    "data_sample = packed_results['data_samples']\n",
    "\n",
    "print('shape of the inputs: ', inputs.shape)\n",
    "\n",
    "# Get metainfo of the inputs\n",
    "print('image_shape: ', data_sample.img_shape)\n",
    "print('num_clips: ', data_sample.num_clips)\n",
    "print('clip_len: ', data_sample.clip_len)\n",
    "\n",
    "# Get label of the inputs\n",
    "print('label: ', data_sample.gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912f3a23-53df-4e65-a21d-c45f8005ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from mmengine.fileio import list_from_file\n",
    "from mmengine.dataset import BaseDataset\n",
    "from mmaction.registry import DATASETS\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class DatasetZelda(BaseDataset):\n",
    "    def __init__(self, ann_file, pipeline, data_root, data_prefix=dict(video=''),\n",
    "                 test_mode=False, modality='RGB', **kwargs):\n",
    "        self.modality = modality\n",
    "        super(DatasetZelda, self).__init__(ann_file=ann_file, pipeline=pipeline, data_root=data_root,\n",
    "                                           data_prefix=data_prefix, test_mode=test_mode,\n",
    "                                           **kwargs)\n",
    "\n",
    "    def load_data_list(self):\n",
    "        data_list = []\n",
    "        fin = list_from_file(self.ann_file)\n",
    "        for line in fin:\n",
    "            line_split = line.strip().split()\n",
    "            if len(line_split) != 2:\n",
    "                print(f\"Skipping invalid line in annotation file: {line}\")\n",
    "                continue\n",
    "            filename, label = line_split\n",
    "            label = int(label)\n",
    "            filename = osp.join(self.data_prefix['video'], filename)\n",
    "            data_list.append(dict(filename=filename, label=label))\n",
    "        return data_list\n",
    "\n",
    "    def get_data_info(self, idx: int) -> dict:\n",
    "        data_info = super().get_data_info(idx)\n",
    "        data_info['modality'] = self.modality\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b7aa60-e004-4996-9883-e133b793786b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fiftyone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Integrate FiftyOne to download and export a subset of the Kinetics-600 dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfiftyone\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfo\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfiftyone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mzoo\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfoz\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Download a subset of the Kinetics-600 dataset\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fiftyone'"
     ]
    }
   ],
   "source": [
    "# Integrate FiftyOne to download and export a subset of the Kinetics-600 dataset\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Download a subset of the Kinetics-600 dataset\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"kinetics-600\",\n",
    "    split=\"validation\",\n",
    "    max_samples=10,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Export the dataset to a directory\n",
    "export_dir = \"data/kinetics600_subset\"\n",
    "dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.VideoClassificationDirectoryTree,\n",
    "    label_field=\"ground_truth\",\n",
    ")\n",
    "\n",
    "# Create annotation files\n",
    "def create_labels_file(split):\n",
    "    labels_file = osp.join(export_dir, f'{split}_labels.txt')\n",
    "    with open(labels_file, 'w') as f:\n",
    "        for sample in dataset.select_fields(['ground_truth']).iter_samples():\n",
    "            video_path = sample.filepath\n",
    "            label = sample.ground_truth.label\n",
    "            f.write(f'{osp.basename(video_path)} {label}\\n')\n",
    "\n",
    "create_labels_file('train')\n",
    "create_labels_file('validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2b51f-e217-458c-9946-3cb7d1aca13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openmmlab)",
   "language": "python",
   "name": "openmmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
